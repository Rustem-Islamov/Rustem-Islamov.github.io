<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">
  
  
  <style>
  a.specialeffects:link{
    color: #303030;
    font-weight: bold;
  }
    a.specialeffects:visited{
    color: #303030;
    font-weight: bold;
  }
    a.specialeffects:active{
    color: #303030;
    font-weight: bold;
  }
    a.specialeffects:hover{
    color: #1F51FF;
    font-weight: bold;
  }
	
  a.specialgray:link{
    color: gray;
  }
    a.specialgray:visited{
    color: gray;
  }
    a.specialgray:active{
    color: gray;
  }
    a.specialgray:hover{
    color: #1F51FF;
  }	
  </style>
  
  
  
    
    
    
  
  

  <meta name="author" content="Rustem Islamov">

  
  
  
    
  
  <meta name="description" content="PhD student">

  
  <link rel="alternate" hreflang="en-us" href="https://rustem-islamov.github.io/post/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Cutive+Mono|Lora:400,700|Roboto:400,700&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.a37180a901f566adcde30f087191d2c2.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-107711380-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="https://www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="/post/index.xml" type="application/rss+xml" title="Rustem Islamov">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://rustem-islamov.github.io/post/">

  
  
  
  
    
    

  

  


  





  <title>Publications | Rustem Islamov</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Rustem Islamov</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        



        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

	<li class="nav-item">
          <a class="nav-link " href="/#education"><span>Education</span></a>
        </li>

	<li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/publication"><span>Publications</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>







<div class="universal-wrapper pt-3">
  <h1 itemprop="name">Prepared in 2024</h1>


<div class="grid-sizer col-lg-12 isotope-item pubtype-0 year-2021">
            <div class="pub-list-item" style="margin-bottom: 1rem" itemscope itemtype="http://schema.org/CreativeWork">
  [10]
  <span itemprop="author" class="article-metadata li-cite-author">
   <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/rustem-islamov/">Rustem Islamov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://www.linkedin.com/in/yuan-gao-1654471b8/?originalSubdomain=de">Yuan Gao</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://www.sstich.ch">Sebastian Stich</a></span>.
  </span>
  <a class="specialeffects" href="/publication/24_motef/" itemprop="name">Near Optimal Decentralized Optimization with Compression and Momentum Tracking</a>.
  <br>

    
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/publications/Motef/motef.pdf" target="_blank" rel="noopener">
  PDF
</a>

<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/24_motef/cite.bib">
  Cite
</button>
    
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2405.20114" target="_blank" rel="noopener">
    arXiv
  </a>



  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" data-toggle="collapse" href="#collapseExample10" role="button" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </a>
  
<div class="collapse" id="collapseExample10">
  <div class="card card-body">
    <font size="2"> Communication efficiency has garnered significant attention as it is considered the main bottleneck for large-scale decentralized Machine Learning applications in distributed and federated settings. In this regime, clients are restricted to transmitting small amounts of quantized information to their neighbors over a communication graph. Numerous endeavors have been made to address this challenging problem by developing algorithms with compressed communication for decentralized non-convex optimization problems. Despite considerable efforts, the current results suffer from various issues such as non-scalability with the number of clients, requirements for large batches, or bounded gradient assumption. In this paper, we introduce MoTEF, a novel approach that integrates communication compression with Momentum Tracking and Error Feedback. Our analysis demonstrates that MoTEF achieves most of the desired properties, and significantly outperforms existing methods under arbitrary data heterogeneity. We provide numerical experiments to validate our theoretical findings and confirm the practical superiority of MoTEF.
      </font>
</div>
</div>

   
</p>

</div>
        </div>






</section>
<section class="header5 cid-rZJT6yYGwi" id="header5-2c">
</div>








<div class="universal-wrapper pt-3">
  <h1 itemprop="name">Prepared in 2023</h1>













<div class="grid-sizer col-lg-12 isotope-item pubtype-0 year-2021">
            <div class="pub-list-item" style="margin-bottom: 1rem" itemscope itemtype="http://schema.org/CreativeWork">
  [9]
  <span itemprop="author" class="article-metadata li-cite-author">
  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://www.linkedin.com/in/yuan-gao-1654471b8/?originalSubdomain=de">Yuan Gao</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/rustem-islamov/">Rustem Islamov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://www.sstich.ch">Sebastian Stich</a></span>.
  </span>
  <a class="specialeffects" href="/publication/23_econtrol/" itemprop="name">EControl: Fast Distributed Optimization with Compression and Error Control</a>.
  <p>
  12th International Conference on Learning Representations (ICLR 2024)<br>
    
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/publications/EControl/econtrol.pdf" target="_blank" rel="noopener">
  PDF
</a>

<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/23_econtrol/cite.bib">
  Cite
</button>
    
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2311.05645" target="_blank" rel="noopener">
    arXiv
  </a>
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://openreview.net/forum?id=lsvlvWB9vz" target="_blank" rel="noopener">
    ICLR
  </a>
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/posters/EControl/Econtrol_poster.pdf" target="_blank" rel="noopener">
    Poster
  </a>

  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" data-toggle="collapse" href="#collapseExample9" role="button" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </a>
  
<div class="collapse" id="collapseExample9">
  <div class="card card-body">
    <font size="2"> Modern distributed training relies heavily on communication compression to reduce the communication overhead. In this work, we study algorithms employing a popular class of contractive compressors in order to reduce communication overhead. However, the naive implementation often leads to unstable convergence or even exponential divergence due to the compression bias. Error Compensation (EC) is an extremely popular mechanism to mitigate the aforementioned issues during the training of models enhanced by contractive compression operators. Compared to the effectiveness of EC in the data homogeneous regime, the understanding of the practicality and theoretical foundations of EC in the data heterogeneous regime is limited. Existing convergence analyses typically rely on strong assumptions such as bounded gradients, bounded data heterogeneity, or large batch accesses, which are often infeasible in modern machine learning applications. We resolve the majority of current issues by proposing EControl, a novel mechanism that can regulate error compensation by controlling the strength of the feedback signal. We prove fast convergence for EControl in standard strongly convex, general convex, and nonconvex settings without any additional assumptions on the problem or data heterogeneity. We conduct extensive numerical evaluations to illustrate the efficacy of our method and support our theoretical findings.
      </font>
</div>
</div>

   
</p>

</div>
        </div>









<div class="grid-sizer col-lg-12 isotope-item pubtype-0 year-2021">
            <div class="pub-list-item" style="margin-bottom: 1rem" itemscope itemtype="http://schema.org/CreativeWork">
  [8]
  <span itemprop="author" class="article-metadata li-cite-author">
  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/rustem-islamov/">Rustem Islamov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://scholar.google.com/citations?user=dJNwgT8AAAAJ&hl=en">Mher Safaryan</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://people.csail.mit.edu/alistarh/">Dan Alistarh</a></span>.
  </span>
  <a class="specialeffects" href="/publication/23_AsyncSGD/" itemprop="name">AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms</a>.
  <p>
      27th International Conference on Artificial Intelligence and Statistics (AISTATS 2024)<br>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/publications/AsyncSGD/AsyncSGD.pdf" target="_blank" rel="noopener">
  PDF
</a>

<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/23_AsyncSGD/cite.bib">
  Cite
</button>
    
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2310.20452" target="_blank" rel="noopener">
    arXiv
  </a>
  
   <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://proceedings.mlr.press/v238/islamov24a.html" target="_blank" rel="noopener">
    AISTATS
  </a>
  
   <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/posters/AsyncSGD/AsyncSGD_poster.pdf" target="_blank" rel="noopener">
    Poster
  </a>

  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" data-toggle="collapse" href="#collapseExample8" role="button" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </a>
  
<div class="collapse" id="collapseExample8">
  <div class="card card-body">
    <font size="2"> We analyze asynchronous-type algorithms for distributed SGD in the heterogeneous setting, where each worker has its own computation and communication speeds, as well as data distribution. In these algorithms, workers compute possibly stale and stochastic gradients associated with their local data at some iteration back in history and then return those gradients to the server without synchronizing with other workers. We present a unified convergence theory for non-convex smooth functions in the heterogeneous regime. The proposed analysis provides convergence for pure asynchronous SGD and its various modifications. Moreover, our theory explains what affects the convergence rate and what can be done to improve the performance of asynchronous algorithms. In particular, we introduce a novel asynchronous method based on worker shuffling. As a by-product of our analysis, we also demonstrate convergence guarantees for gradient-type algorithms such as SGD with random reshuffling and shuffle-once mini-batch SGD. The derived rates match the best-known results for those algorithms, highlighting the tightness of our approach. Finally, our numerical evaluations support theoretical findings and show the good practical performance of our method.
  </font>
</div>
</div>

   
</p>

</div>
        </div>









<div class="grid-sizer col-lg-12 isotope-item pubtype-0 year-2021">
            <div class="pub-list-item" style="margin-bottom: 1rem" itemscope itemtype="http://schema.org/CreativeWork">
  [7]
  <span itemprop="author" class="article-metadata li-cite-author">
   <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://sarit-khirirat.netlify.app">Sarit Khirirat</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/rustem-islamov/">Rustem Islamov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://scholar.google.ca/citations?user=9_Hpd5kAAAAJ&hl=en">Fakhri Karray</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://richtarik.org">Peter Richtárik</a></span>.
  </span>
  <a class="specialeffects" href="/publication/23_clip21/" itemprop="name">Clip21: Error Feedback for Gradient Clipping</a>.
  <p>
    
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/publications/Clip21/clip21.pdf" target="_blank" rel="noopener">
  PDF
</a>

<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/23_clip21/cite.bib">
  Cite
</button>
    
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2305.18929" target="_blank" rel="noopener">
    arXiv
  </a>

  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" data-toggle="collapse" href="#collapseExample7" role="button" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </a>
  
<div class="collapse" id="collapseExample7">
  <div class="card card-body">
    <font size="2"> Motivated by the increasing popularity and importance of large-scale training under differential privacy (DP) constraints, we study distributed gradient methods with gradient clipping, i.e., clipping applied to the gradients computed from local information at the nodes. While gradient clipping is an essential tool for injecting formal DP guarantees into gradient-based methods [Abadi et al., 2016], it also induces bias which causes serious convergence issues specific to the distributed setting. Inspired by recent progress in the error-feedback literature which is focused on taming the bias/error introduced by communication compression operators such as Top-k [Richtárik et al., 2021], and mathematical similarities between the clipping operator and contractive compression operators, we design Clip21 -- the first provably effective and practically useful error feedback mechanism for distributed methods with gradient clipping. We prove that our method converges at the same ${\tiny \mathcal{O}(1/K)}$ rate as distributed gradient descent in the smooth nonconvex regime, which improves the previous best ${\tiny \mathcal{O}(1/\sqrt{K})}$ rate which was obtained under significantly stronger assumptions. Our method converges significantly faster in practice than competing methods.
  </font>
</div>
</div>

   
</p>

</div>
        </div>






<div class="grid-sizer col-lg-12 isotope-item pubtype-0 year-2021">
            <div class="pub-list-item" style="margin-bottom: 1rem" itemscope itemtype="http://schema.org/CreativeWork">
  [6]
  <span itemprop="author" class="article-metadata li-cite-author">
  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://www.konstmish.com">Konstantin Mishchenko</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/rustem-islamov/">Rustem Islamov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://sites.google.com/view/samuelhorvath">Samuel Horváth</a>.</span>
  </span>
  <a class="specialeffects" href="/publication/23_partial/" itemprop="name">Partially Personalized Federated Learning: Breaking the Curse of Data Heterogeneity</a>.
  <p>
    
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/publications/Partial/partial.pdf" target="_blank" rel="noopener">
  PDF
</a>

<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/23_partial/cite.bib">
  Cite
</button>
    
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2305.18285" target="_blank" rel="noopener">
    arXiv
  </a>

  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" data-toggle="collapse" href="#collapseExample6" role="button" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </a>
<div class="collapse" id="collapseExample6">
  <div class="card card-body">
    <font size="2"> We present a partially personalized formulation of Federated Learning (FL) that strikes a balance between the flexibility of personalization and cooperativeness of global training. In our framework, we split the variables into global parameters, which are shared across all clients, and individual local parameters, which are kept private. We prove that under the right split of parameters, it is possible to find global parameters that allow each client to fit their data perfectly, and refer to the obtained problem as overpersonalized. For instance, the shared global parameters can be used to learn good data representations, whereas the personalized layers are fine-tuned for a specific client. Moreover, we present a simple algorithm for the partially personalized formulation that offers significant benefits to all clients. In particular, it breaks the curse of data heterogeneity in several settings, such as training with local steps, asynchronous training, and Byzantine-robust training.
  </font>
</div>
</div>

   
</p>

</div>
        </div>


</section>
<section class="header5 cid-rZJT6yYGwi" id="header5-2c">
</div>

  










<div class="universal-wrapper pt-3">
  <h1 itemprop="name">Prepared in 2022</h1>





<div class="grid-sizer col-lg-12 isotope-item pubtype-0 year-2021">
            <div class="pub-list-item" style="margin-bottom: 1rem" itemscope itemtype="http://schema.org/CreativeWork">
  [5]
  <span itemprop="author" class="article-metadata li-cite-author">
  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://scholar.google.com/citations?user=jXPpMXkAAAAJ&hl=en">Maksim Makarenko</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://elnurgasanov.com">Elnur Gasanov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/rustem-islamov/">Rustem Islamov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://sadiev.netlify.app">Abdurakhmon Sadiev</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://richtarik.org">Peter Richtárik</a>.</span>
  </span>
  <a class="specialeffects" href="/publication/22_ada3pc/" itemprop="name">Adaptive Compression for Communication-Efficient Distributed Training</a>.
  <p>
  Transactions on Machine Learning Research (TMLR 2023)<br>
    
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/publications/Ada3PC/ada3pc.pdf" target="_blank" rel="noopener">
  PDF
</a>

<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/22_ada3pc/cite.bib">
  Cite
</button>
    
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2211.00188" target="_blank" rel="noopener">
    arXiv
  </a>
  
    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://openreview.net/forum?id=Rb6VDOHebB" target="_blank" rel="noopener">
    TMLR
  </a>
  
 <a class="btn btn-outline-primary my-1 mr-1 btn-sm" data-toggle="collapse" href="#collapseExample5" role="button" aria-expanded="false" aria-controls="collapseExample">    Abstract
  </a>
<div class="collapse" id="collapseExample5">
  <div class="card card-body">
    <font size="2"> We propose Adaptive Compressed Gradient Descent (AdaCGD) - a novel optimization algorithm for communication-efficient training of supervised machine learning models with adaptive compression level. Our approach is inspired by the recently proposed three point compressor (3PC) framework of Richtarik et al. (2022), which includes error feedback (EF21), lazily aggregated gradient (LAG), and their combination as special cases, and offers the current state-of-the-art rates for these methods under weak assumptions. While the above mechanisms offer a fixed compression level, or adapt between two extremes only, our proposal is to perform a much finer adaptation. In particular, we allow the user to choose any number of arbitrarily chosen contractive compression mechanisms, such as Top-K sparsification with a user-defined selection of sparsification levels K, or quantization with a user-defined selection of quantization levels, or their combination. AdaCGD chooses the appropriate compressor and compression level adaptively during the optimization process. Besides i) proposing a theoretically-grounded multi-adaptive communication compression mechanism, we further ii) extend the 3PC framework to bidirectional compression, i.e., we allow the server to compress as well, and iii) provide sharp convergence bounds in the strongly convex, convex and nonconvex settings. The convex regime results are new even for several key special cases of our general mechanism, including 3PC and EF21. In all regimes, our rates are superior compared to all existing adaptive compression methods.
  </font>
</div>
</div>

   
</p>

</div>
        </div>





<div class="grid-sizer col-lg-12 isotope-item pubtype-0 year-2021">
            <div class="pub-list-item" style="margin-bottom: 1rem" itemscope itemtype="http://schema.org/CreativeWork">
  [4]
  <span itemprop="author" class="article-metadata li-cite-author">
  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/rustem-islamov/">Rustem Islamov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://scholar.google.com/citations?user=2QtdkysAAAAJ&hl=en">Xun Qian</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://slavomir-hanzely.github.io">Slavomir Hanzely</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://scholar.google.com/citations?user=dJNwgT8AAAAJ&hl=en">Mher Safaryan</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://richtarik.org">Peter Richtárik</a>.</span>

  </span>
  <a class="specialeffects" href="/publication/22_3pc/" itemprop="name">Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation</a>.
  <p>
  Transactions on Machine Learning Research (TMLR 2023)<br>
  
    
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/publications/3PC/3PC.pdf" target="_blank" rel="noopener">
  PDF
</a>

<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/22_3pc/cite.bib">
  Cite
</button>
    
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2206.03588" target="_blank" rel="noopener">
    arXiv
  </a>
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://openreview.net/forum?id=NekBTCKJ1H" target="_blank" rel="noopener">
    TMLR
  </a>

  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/posters/Newton-3PC/Newton3PC-poster.pdf" target="_blank" rel="noopener">
    poster
  </a>

  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://order-up-ml.github.io/" target="_blank" rel="noopener">
    HOO-22 NeurIPS
  </a>

  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" data-toggle="collapse" href="#collapseExample4" role="button" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </a>
<div class="collapse" id="collapseExample4">
  <div class="card card-body">
    <font size="2">Despite their high computation and communication costs, Newton-type methods remain an appealing option for distributed training due to their robustness against ill-conditioned convex problems. In this work, we study communication compression and aggregation mechanisms for curvature information in order to reduce these costs while preserving theoretically superior local convergence guarantees. We prove that the recently developed class of three point compressors (3PC) of Richtarik et al. [2022] for gradient communication can be generalized to Hessian communication as well. This result opens up a wide variety of communication strategies, such as contractive compression and lazy aggregation, available to our disposal to compress prohibitively costly curvature information. Moreover, we discovered several new 3PC mechanisms, such as adaptive thresholding and Bernoulli aggregation, which require reduced communication and occasional Hessian computations. Furthermore, we extend and analyze our approach to bidirectional communication compression and partial device participation setups to cater to the practical considerations of applications in federated learning. For all our methods, we derive fast condition-number-independent local linear and/or superlinear convergence rates. Finally, with extensive numerical evaluations on convex optimization problems, we illustrate that our designed schemes achieve state-of-the-art communication complexity compared to several key baselines using second-order information.
</font>  
</div>
</div>


   
</p>

</div>
        </div>







</section>

<section class="header5 cid-rZJT6yYGwi" id="header5-2c">


</div>








  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1 itemprop="name">Prepared in 2021</h1>



<div class="grid-sizer col-lg-12 isotope-item pubtype-0 year-2021">
            <div class="pub-list-item" style="margin-bottom: 1rem" itemscope itemtype="http://schema.org/CreativeWork">
  [3]
  <span itemprop="author" class="article-metadata li-cite-author">
  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://scholar.google.com/citations?user=2QtdkysAAAAJ&hl=en">Xun Qian</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/rustem-islamov/">Rustem Islamov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://scholar.google.com/citations?user=dJNwgT8AAAAJ&hl=en">Mher Safaryan</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://richtarik.org">Peter Richtárik</a>.</span>

  </span>
  <a class="specialeffects" href="/publication/21_bl/" itemprop="name">Basis Matters: Better Communication-Efficient Second Order Methods for Federated Learning</a>.
  
  <p>
  25th International Conference on Artificial Intelligence and Statistics (AISTATS 2022)<br>
    
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/publications/BL/bl.pdf" target="_blank" rel="noopener">
  PDF
</a>

<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/21_bl/cite.bib">
  Cite
</button>
    
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2111.01847" target="_blank" rel="noopener">
    arXiv
  </a>

<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://proceedings.mlr.press/v151/qian22a.html" target="_blank" rel="noopener">
    AISTATS
  </a>
 
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/posters/BasisLearn/basis_learn_poster.pdf" target="_blank" rel="noopener">
    Poster
  </a>


   <a class="btn btn-outline-primary my-1 mr-1 btn-sm" data-toggle="collapse" href="#collapseExample3" role="button" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </a>
<div class="collapse" id="collapseExample3">
  <div class="card card-body">
    <font size="2">Recent advances in distributed optimization have shown that Newton-type methods with proper communication compression mechanisms can guarantee fast local rates and low communication cost compared to first order methods. We discover that the communication cost of these methods can be further reduced, sometimes dramatically so, with a surprisingly simple trick: Basis Learn (BL). The idea is to transform the usual representation of the local Hessians via a change of basis in the space of matrices and apply compression tools to the new representation. To demonstrate the potential of using custom bases, we design a new Newton-type method (BL1), which reduces communication cost via both BL technique and bidirectional compression mechanism. Furthermore, we present two alternative extensions (BL2 and BL3) to partial participation to accommodate federated learning applications. We prove local linear and superlinear rates independent of the condition number. Finally, we support our claims with numerical experiments by comparing several first and second-order methods.
  </font></div>
</div>

   
</p>

</div>
        </div>

</section>

<section class="header5 cid-rZJT6yYGwi" id="header5-2c">



<div class="grid-sizer col-lg-12 isotope-item pubtype-0 year-2021">
            <div class="pub-list-item" style="margin-bottom: 1rem" itemscope itemtype="http://schema.org/CreativeWork">
  [2]
  <span itemprop="author" class="article-metadata li-cite-author">
  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://scholar.google.com/citations?user=dJNwgT8AAAAJ&hl=en">Mher Safaryan</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/rustem-islamov/">Rustem Islamov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://scholar.google.com/citations?user=2QtdkysAAAAJ&hl=en">Xun Qian</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://richtarik.org">Peter Richtárik</a>.</span>

  </span>
  <a class="specialeffects" href="/publication/21_fednl/" itemprop="name">FedNL: Making Newton-Type Methods Applicable to Federated Learning</a>.
  
  <p>
  39th International Conference on Machine Learning (ICML 2022)<br>
    
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/publications/FedNL/FedNL.pdf" target="_blank" rel="noopener">
  PDF
</a>

<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/21_fednl/cite.bib">
  Cite
</button>
    
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2106.02969" target="_blank" rel="noopener">
    arXiv
  </a>

 <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://proceedings.mlr.press/v162/safaryan22a.html" target="_blank" rel="noopener">
    ICML
  </a>





<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://federated-learning.org/fl-icml-2021/" target="_blank" rel="noopener">
    FL-ICML
  </a>


<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://sites.google.com/view/optml-icml2021/accepted-papers?authuser=0" target="_blank" rel="noopener">
    BFOM-ICML
  </a>

<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/posters/FedNL/FedNL_poster.pdf" target="_blank" rel="noopener">
    Poster
  </a>

<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.youtube.com/watch?v=eAGqO4pYEW4" target="_blank" rel="noopener">
    3-min video
  </a>


  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" data-toggle="collapse" href="#collapseExample2" role="button" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </a>
<div class="collapse" id="collapseExample2">
  <div class="card card-body">
    <font size="2">Inspired by recent work of Islamov et al (2021), we propose a family of Federated Newton Learn (FedNL) methods, which we believe is a marked step in the direction of making second-order methods applicable to FL. In contrast to the aforementioned work, FedNL employs a different Hessian learning technique which i) enhances privacy as it does not rely on the training data to be revealed to the coordinating server, ii) makes it applicable beyond generalized linear models, and iii) provably works with general contractive compression operators for compressing the local Hessians, such as Top-K or Rank-R, which are vastly superior in practice. Notably, we do not need to rely on error feedback for our methods to work with contractive compressors. Moreover, we develop FedNL-PP, FedNL-CR and FedNL-LS, which are variants of FedNL that support partial participation, and globalization via cubic regularization and line search, respectively, and FedNL-BC, which is a variant that can further benefit from bidirectional compression of gradients and models, i.e., smart uplink gradient and smart downlink model compression. We prove local convergence rates that are independent of the condition number, the number of training data points, and compression variance. Our communication efficient Hessian learning technique provably learns the Hessian at the optimum. Finally, we perform a variety of numerical experiments that show that our FedNL methods have state-of-the-art communication complexity when compared to key baselines.
</font>  
</div>
</div>



     
</p>
</div>
        </div>






<div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            <div class="pub-list-item" style="margin-bottom: 1rem" itemscope itemtype="http://schema.org/CreativeWork">
  [1]
  <span itemprop="author" class="article-metadata li-cite-author">
  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/rustem-islamov/">Rustem Islamov</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://scholar.google.com/citations?user=2QtdkysAAAAJ&hl=en">Xun Qian</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="https://richtarik.org">Peter Richtárik</a>.</span>
  </span>
  <a class="specialeffects" href="/publication/21_mn/" itemprop="name">Distributed Second Order Methods with Fast Rates and Compressed Communication</a>.
  
  <p>

38th International Conference on Machine Learning (ICML 2021)<br>

<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/publications/NEWTON-LEARN/NEWTON-LEARN.pdf" target="_blank" rel="noopener">
  PDF
</a>
<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/21_mn/cite.bib">
  Cite
</button>
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2102.07158" target="_blank" rel="noopener">
    arXiv
  </a>


<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://proceedings.mlr.press/v139/islamov21a.html" target="_blank" rel="noopener">
    
    ICML
  </a>


<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://sites.google.com/ucsd.edu/cedo/posters?authuser=0" target="_blank" rel="noopener">
    NSF-TRIPODS
  </a>

    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/posters/NEWTON-LEARN/Newton_Learn_Poster.pdf" target="_blank" rel="noopener">
    Poster
  </a>

  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.youtube.com/watch?v=iSKBZXlaoWo&list=PLC28kDljnOrj-_w-MHKW36gVRvUe3XFjx&index=43" target="_blank" rel="noopener">
    
    5-min video
  </a>


  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" data-toggle="collapse" href="#collapseExample1" role="button" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </a>
<div class="collapse" id="collapseExample1">
  <div class="card card-body">
    <font size="2">We develop several new communication-efficient second-order methods for distributed optimization. Our first method, NEWTON-STAR, is a variant of Newton’s method from which it inherits its fast local quadratic rate. However, unlike Newton’s method, NEWTON-STAR enjoys the same per iteration communication cost as gradient descent. While this method is impractical as it relies on the use of certain unknown parameters characterizing the Hessian of the objective function at the optimum, it serves as the starting point which enables us design practical variants thereof with strong theoretical guarantees. In particular, we design a stochastic sparsification strategy for learning the unknown parameters in an iterative fashion in a communication efficient manner. Applying this strategy to NEWTON-STAR leads to our next method, NEWTON-LEARN, for which we prove local linear and superlinear rates independent of the condition number. When applicable, this method can have dramatically superior convergence behavior when compared to state-of-the-art methods. Finally, we develop a globalization strategy using cubic regularization which leads to our next method, CUBIC-NEWTON-LEARN, for which we prove global sublinear and linear convergence rates, and a fast superlinear rate. Our results are supported with experimental results on real datasets, and show several orders of magnitude improvement on baseline and state-of-the-art methods in terms of communication complexity.
</font>  
</div>
</div>

</p>
</div>
        </div>






</section>

<section class="header5 cid-rZJT6yYGwi" id="header5-2c">











  





  


    
  
    
      










  





  

    
  
    
      










  





  



    
  
    
      










  





  

    
  
    
      










  





  


    
  
    
      










  





  


  
    
      










  





  



    
  

 



</div>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="https://rustem-islamov.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.16bbb3750feb7244c9bc409a5a4fe678.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2024 Rustem Islamov&middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
